{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>This is the main, common repository of the RS-DAT project. Any common issues can and should be raised here, and common developments can be housed here. Issues and solutions arsing from the working groups that may/will affect the other usecases should be allso be linked from here.</p>"},{"location":"examples/","title":"Examples","text":"<p>Access to the notebooks</p> <p></p> 01-Work with STAC Catalogs on the dCache Storage 02-Filter large AHN3 point-cloud files 03-Phenology use-case for RS-DAT 04-Machine learning use-case for RS-DAT"},{"location":"examples/#work-with-stac-catalogs-on-the-dcache-storage","title":"Work with STAC Catalogs on the dCache Storage","text":"<p>This example includes two Jupyter notebooks that illustrate how to: * search for scenes from the Sentinel-2 mission as part of a open dataset on AWS * save the metadata in the form of a SpatioTemporal Asset Catalog on the SURF dCache storage system. * retrieve some of the scenes' assets. * doing some simple processing on the retrieved assets using a Dask cluster to distribute workload.</p>"},{"location":"examples/#additional-dependencies","title":"Additional dependencies","text":"<p>The example requires the packages listed in the conda environment file provided in this folder.  </p>"},{"location":"examples/#filter-large-ahn3-point-cloud-files","title":"Filter large AHN3 point-cloud files","text":"<p>This example includes a Jupyter notebook that illustrates how to filter/sample large LAZ files from the Actueel Hoogtebestand Nederland (AHN) - version 3 dowloaded to the SURF dCache storage. </p>"},{"location":"examples/#additional-dependencies_1","title":"Additional dependencies","text":"<p>The example requires the packages listed in the conda environment file provided in this folder.  </p>"},{"location":"examples/#phenology-use-case-for-rs-dat","title":"Phenology use-case for RS-DAT","text":"<p>This use case has moved to a dedicated repository: https://github.com/RS-DAT/Phenology</p>"},{"location":"examples/#_2","title":"Examples","text":"<p>There are two notebooks created for running a machine learning prediction problem on two datasets with different size. Depending on the infra e.g. SURF research cloud, or Snellius, the I/O paths and reading, writing files can be different.</p> <p>Big data is only available on snellius at <code>/gpfs/work2/0/ttse0619/DAT_data</code> whereas Small data on dcache. </p>"},{"location":"how_to_contribute/","title":"Contributing","text":""},{"location":"how_to_contribute/#submit-an-issue","title":"Submit an issue","text":"<p>The repository <code>RS-DAT</code> is a meta repository in the RS-DAT GitHub organization. To deply RS-DAT, have a look at the RS-DAT documentation. We welcome any kind of contribution to RS-DAT, from simple comment or question to a use case, or a pull request. A contribution can be one of the following cases:</p> <ol> <li>you have a question;</li> <li>you think you may have found a bug (including unexpected behavior);</li> <li>you want to make some kind of change to the code base (e.g. to fix a bug, to    add a new feature, to update documentation);</li> <li>you want to discuss your use case or add the use case to    Examples.</li> </ol> <p>In all cases, you can start by opening an issue in RS-DAT.</p>"},{"location":"how_to_contribute/#contact-us","title":"Contact us","text":"<p>See RS-DAT project page.</p>"},{"location":"what_license/","title":"License","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <pre><code>  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n</code></pre> <p>Copyright [yyyy] [name of copyright owner]</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <pre><code>   http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p>"},{"location":"demos/023_05_22_scalable_GIS/","title":"023-05-22-scalable-GIS","text":"<p>Access the source code</p> <p></p>"},{"location":"demos/023_05_22_scalable_GIS/#overview","title":"Overview","text":"<p>This repository contains material for the second session of the workshop Scalable GIS for Urbanists carried out at TU Delft. </p> <p>The workshop run on the Spider Data Processing infrastructure at SURF using the setup described in this repository.</p> <p>See the Setup document for setup instructions.</p>"},{"location":"demos/023_05_22_scalable_GIS/#links-credits","title":"Links &amp; Credits","text":"<ul> <li>Rbanism Netherlands eScience Center fellowship project</li> <li>RS-DAT project page</li> </ul>"},{"location":"demos/2022_12_01_demo_remote_sensing_session/","title":"2022-12-01-demo-remote-sensing-session","text":"<p>Access the source code</p> <p></p>"},{"location":"demos/2022_12_01_demo_remote_sensing_session/#overview","title":"Overview","text":"<p>This repository contains material for the demo of the Remote-Sensing Deployable Analysis environmenT (RS-DAT), to take place in the Remote Sensing Session at the Department of Physical Geography (Utrecht University) on 2022-12-01. </p>"},{"location":"demos/2022_12_01_demo_remote_sensing_session/#acknowledgement","title":"Acknowledgement","text":"<p>We thank Prof. Raul Zurita Milla and Dr. Emma Izquierdo-Verdiguer for sharing knowledge and material to set up the phenology use case.  </p>"},{"location":"demos/2023_03_10_demo_TU_Delft/","title":"2023-03-10-demo-TU-Delft","text":"<p>Access the source code</p> <p></p>"},{"location":"demos/2023_03_10_demo_TU_Delft/#overview","title":"Overview","text":"<p>This repository contains material for the demo of the Remote-Sensing Deployable Analysis environmenT (RS-DAT), to take place at TU Delft on 2023-03-10. The notebooks in the <code>examples</code> directory, are run on the Spider Data Processing infrastructure at SURF using the setup described in this repository.</p>"},{"location":"demos/2023_03_10_demo_TU_Delft/#links-credits","title":"Links &amp; Credits","text":"<ul> <li>RS-DAT project page.</li> <li> <p>The presented use cases are based on the following collaborative projects:</p> </li> <li> <p>Phenology, with Prof. Raul Zurita Milla (Universiteit Twente - ITC) and Dr. Emma Izquierdo-Verdiguer (BOKU, Vienna).</p> </li> <li>Motion by Learning (MobyLe), with Dr.ir. Freek van Leijen.</li> </ul>"},{"location":"demos/2023_05_15_workshop_ITC/","title":"2023-05-15-workshop-ITC","text":"<p>Access the source code</p> <p></p>"},{"location":"demos/2023_05_15_workshop_ITC/#overview","title":"Overview","text":"<p>This repository contains material for the workshop carried out at the Faculty of Geo-information Science and Earth Observation (ITC) of University of Twente. </p> <p>The workshop run on the Spider Data Processing infrastructure at SURF using the setup described in this repository.</p>"},{"location":"demos/2023_05_15_workshop_ITC/#links-credits","title":"Links &amp; Credits","text":"<ul> <li>Workshop event page</li> <li>RS-DAT project page</li> </ul> <p>The presented use cases are based on the following collaborative projects:</p> <ul> <li>Phenology, with Prof. Raul Zurita Milla (Universiteit Twente - ITC) and Dr. Emma Izquierdo-Verdiguer (BOKU, Vienna).</li> </ul>"},{"location":"demos/2023_05_31_SummerSchool_HDCRS/","title":"2023-05-31-SummerSchool-HDCRS","text":"<p>Access the source code</p> <p></p>"},{"location":"demos/2023_05_31_SummerSchool_HDCRS/#overview","title":"Overview","text":"<p>This repository contains material for the Remote Sensing Data Analysis using HTC/HPC Systems session to be presented at the HDCRS summer school at the University of Iceland, Reykjavik on 2023-05-31.</p> <p>The notebooks in the <code>code-along-notebooks</code> directory will be run on the Spider Data Processing infrastructure at SURF using the setup described in this repository.</p> <p>See the Setup document for the local system requirements and installation instructions.</p> <p>This repository also contains the presentation slides and a copy of the collaborative document employed during the course. </p>"},{"location":"demos/2023_05_31_SummerSchool_HDCRS/#links-credits","title":"Links &amp; Credits","text":"<ul> <li>RS-DAT project page.</li> <li> <p>The presented use cases are based on the following collaborative projects:</p> </li> <li> <p>Phenology, with Prof. Raul Zurita Milla (Universiteit Twente - ITC) and Dr. Emma Izquierdo-Verdiguer (BOKU, Vienna).</p> </li> <li>Motion by Learning (MobyLe), with Dr.ir. Freek van Leijen.</li> </ul>"},{"location":"deploying/jupyter_dask_cloud_to_cluster/","title":"Jupyter Dask Cloud to Cluster","text":"<p>Access the source code</p> <p></p> <p>Material to deploy Jupyter and Dask on SRC and SLURM cluster</p>"},{"location":"deploying/jupyter_dask_on_slurm/","title":"Jupyter Dask On SLURM","text":"<p>Access the source code</p> <p></p> <p>How to cite</p> <p></p> <p>This repository contains instructions and material to setup and run a Jupyter server and a Dask cluster on a SLURM system, such as the Spider data processing platform and the Snellius supercomputer, both hosted by SURF.</p> <p>Have a look at the User Guide for detailed instructions on how to get started.</p>"},{"location":"deploying/jupyter_dask_on_slurm/#examples","title":"Examples","text":"<ul> <li>Example use cases making use of this deployment can be found here. </li> <li>The material employed for a demo, including tutorial videos is available here.</li> </ul>"},{"location":"deploying/jupyter_dask_on_slurm/#resources","title":"Resources","text":"<ul> <li>Getting Started with Pangeo on HPC</li> <li>Interactive Use \u2014 Dask-jobqueue</li> <li>Jupyter on the HPC Clusters | Princeton Research Computing</li> </ul>"},{"location":"deploying/jupyter_dask_on_src/","title":"Jupyter Dask On SRC","text":"<p>Access the source code</p> <p></p> <p>Deploy Dask on a cluster of virtual machines (VMs) on SURF Research Cloud (SRC). Access the cluster via JupyterHub, deployed on the head node.</p>"},{"location":"deploying/jupyter_dask_on_src/#create-the-src-catalog-item","title":"Create the SRC Catalog Item","text":"<p>A member of the CO with \"developer\" privileges can register the current application as a Catalog Item on SURF Research Cloud following these instructions.</p> <p>The following steps create a catalog item with the following three paraters: * the number of worker nodes in the cluster of VMs; * (optional) the URL to a conda environment file, employed to configure both the head node and the workers with the same Python environment; * (optional) a macaroon to authenticate access to the SURF dCache starage via dCacheFS.</p> <p>Note that setting up the \"Catalog Item\" requires first to create a \"Component\" based on the Ansible playbook that is part of this repository.</p>"},{"location":"deploying/jupyter_dask_on_src/#create-the-component","title":"Create the Component","text":"<p>Follow the \"Create a component\" instructions, using the following parameters:</p> <ul> <li>Set \"Component script type\" to \"Ansible Playbook\".</li> <li>In the \"Script source\" tab:</li> <li>Set \"Repository URL\" as <code>https://github.com/RS-DAT/JupyterDaskOnSRC.git</code>;</li> <li>Set \"Path\" as <code>research-cloud-plugin.yml</code>;</li> <li>(if neededed) Set \"Tag\" as target \"branch\" or \"tag\";</li> <li>Set \"Script availability\" as \"Script is publicly available through Git\". </li> <li>Add three \"Component parameters\":</li> <li>Set \"Parameter key\" as <code>environment_url</code>, \"Source type\" as \"Fixed\", and tick \"Overwritable\";</li> <li>Set \"Parameter key\" as <code>dcache_token</code>, \"Source type\" as \"Fixed\", and tick \"Overwritable\";</li> <li>Set \"Parameter key\" as <code>worker_ip_addresses</code>, \"Source type\" as \"Resource\".</li> </ul>"},{"location":"deploying/jupyter_dask_on_src/#create-the-catalog-item","title":"Create the Catalog Item","text":"<p>Follow the \"Create a catalog item\" instructions, using the following parameters:</p> <ul> <li>In the \"Available components\" tab, choose the following components (in this order):</li> <li>SRC-OS</li> <li>SRC-CO</li> <li>SRC-Ngnix</li> <li>SRC-External plugin</li> <li>the component created in the previous step</li> <li>In the \"Cloud providers\" tab, add \"SURF HPC Cloud cluster\" with \"Ubuntu 20.04\" as OS and tick all available sizes.</li> <li>In the \"Ovewritable parameters\" tab:</li> <li>For <code>co_passwordless_sudo</code>, set \"Actions\" to \"Overwrite\" and set \"New parameter value\" to <code>true</code>.</li> <li>For <code>num_nodes</code>, set \"Actions\" to \"Make interactive\", set \"Label or question for workspace creator\" to \"Number of worker nodes\", and set \"Default parameter value\" to <code>0</code>.</li> <li>For <code>dcache_token</code>, set \"Actions\" to \"Make interactive\", set \"Label or question for workspace creator\" to \"dCache token: provide the macaroon to authenticate on the dCache storage\", and set \"Default parameter value\" to <code>null</code>.</li> <li>For <code>environment_url</code>, set \"Actions\" to \"Make interactive\", set \"Label or question for workspace creator\" to \"Conda environment file: provide the URL to a custom environment.yml (blank for default environment)\", and set \"Default parameter value\" to <code>default</code>.</li> </ul>"},{"location":"deploying/jupyter_dask_on_src/#credits","title":"Credits","text":"<p>Most of the playbooks in this repository have been taken, inspired or adapted from the ones in the eWaterCycle infrastructure and the Emma projects. </p>"},{"location":"deploying/jupyter_docker_spawner_on_src/","title":"Jupyter Docker Spawner On SRC","text":"<p>Access the source code</p> <p></p> <p>Deploy JupyterHub with DockerSpawner on SURF Research Cloud (SRC).</p> <p>The JupyterHub Docker Spawner enables JupyterHub to spawn single user notebook servers in Docker containers.</p> <p>This document illustrates how to create a component, a catalog item and a workspace on SRC.</p>"},{"location":"deploying/jupyter_docker_spawner_on_src/#create-a-component","title":"Create a component","text":"<p>Follow the SRC Create a component instructions and in the step <code>Define component script and source</code>, set the source as <code>https://github.com/RS-DAT/JupyterDockerSpawnerOnSRC.git</code>.</p> <p>See each item below for more technical information about the component:</p> <ul> <li>research-cloud-plugin.yml for a list of   packages that are installed.</li> <li>jupyterhub_config.py.j2   for a list of configs for jupyterhub and docker spawner.</li> </ul>"},{"location":"deploying/jupyter_docker_spawner_on_src/#create-a-catalog-item","title":"Create a catalog item","text":"<p>Follow the SRC Create a catalog item and choose the components as below:</p> <ul> <li>SRC-OS</li> <li>SRC-CO</li> <li>SRC-Ngnix</li> <li>SRC-External plugin</li> <li>the JupyterDockerSpawnerOnSRC component created in the previous step</li> </ul> <p>Note: When choosing the <code>Operating system</code> in the step <code>Cloud provider</code>, choose Ubuntu 20, because the latest version of Ubuntu does not support <code>iRODS</code> that is needed for <code>SRC-CO</code> component.</p>"},{"location":"deploying/jupyter_docker_spawner_on_src/#create-a-workspace","title":"Create a workspace","text":"<p>Follow the SRC Start a simple workspace and choose the item that is created in the previous step.</p>"},{"location":"deploying/jupyter_docker_spawner_on_src/#docker-images-and-containers","title":"Docker images and containers","text":"<p>The docker image can be set in the jupyterhub config file. After logging to Jupyter server, the docker image is pulled and a container starts running. The volumes <code>/data</code> and <code>/scratch</code> are mounted within the user home directory (<code>~/data</code> and <code>~/scratch</code>) inside the container. If the user does not stop the server, the container continues running until a default timeout is reached (see Jupyter parameters here). Otherwise, the container will be stopped (Status Exited). Next time the user login, the same container will be used. So, the data and packages are preserved across sessions. Note that, the container and images will stay on the workspace. Only when the Jupyter service is stopped, the containers will be removed.</p>"},{"location":"deploying/jupyter_docker_spawner_on_src/#remove-images-or-containers","title":"Remove images or containers","text":"<p>You can log in to the workspace using <code>ssh</code> and use docker command with <code>sudo</code> to delete images/containers.</p>"},{"location":"deploying/jupyter_docker_spawner_on_src/#credits","title":"Credits","text":"<p>Most of the playbooks in this repository have been taken, inspired or adapted from the ones in the RS-DAT JupyterDaskOnSRC and in the eWaterCycle infra.</p>"},{"location":"deploying/jupyterdask_with_singularity/","title":"Jupyter Dask with Singularity","text":"<p>Access the source code</p> <p></p> <p>This repository describe work in progress aimed at running Jupyter and Dask using Singularity containers. The following steps have been run on the Spider data-processing platform at SURF. </p>"},{"location":"deploying/jupyterdask_with_singularity/#docker-images","title":"Docker images","text":"<p>We use a custom image created using repo2docker and hosted on GitHub Packages (see corresponding repository). On the SLURM system (Spider), we download the image and convert it to Singularity (now Apptainer): <pre><code>apptainer build test-jupyterdask-image.sif docker://ghcr.io/fnattino/test-jupyterdask-image:latest\n</code></pre></p> <p>We submit a job that start JupyterLab (and the Dask scheduler) in a container on a compute node: <pre><code>sbatch jupyter.slurm ./test-jupyterdask-image.sif\n</code></pre></p> <p>(Note that the two commmands above can also be run as a single step: <code>sbatch jupyter.slurm docker://ghcr.io/fnattino/test-jupyterdask-image:latest</code>)</p> <p>On the SLURM system (Spider), workers can be \"manually\" added to the Dask cluster using: <pre><code># get the scheduler address from the Jupyter session\nsbatch dask-worker.slurm ./test-jupyterdask-image.sif tcp://10.0.0.XX:XXXXX \n</code></pre></p>"},{"location":"deploying/jupyterdask_with_singularity/#singularity-containers","title":"Singularity containers","text":"<p>We have built an example singularity container hosted on GitHub Packages in this repository. The advantage of using native singularity files is that we can skip the conversion to SIF format.</p> <p>Starting JupyterLab in a container on a compute node: <pre><code>sbatch jupyter.slurm oras://ghcr.io/fnattino/test-jupyterdask-image-apptainer:latest\n</code></pre></p>"},{"location":"deploying/jupyterdask_with_singularity/#port-forwarding","title":"Port forwarding","text":"<p>On your local system run port forwarding using the command printed in the SLURM stdout file, then  access Jupyter from the web browser at http://localhost:8889/lab</p>"},{"location":"deploying/jupyterdask_with_singularity/#resources","title":"Resources","text":"<ul> <li>https://github.com/pbranson/pangeo-hpc-singularity/tree/master</li> <li>https://gist.github.com/willirath/2176a9fa792577b269cb393995f43dda</li> <li>https://github.com/ESM-VFC/esm-vfc-stacks/tree/master</li> </ul>"}]}